---
description: "Halluzinationen sind ein Risiko, das beim Einsatz von KI auftreten kann, wenn KI-Systeme falsche oder ungenaue Informationen erzeugen."
id: halluzinationen
tags:
- falsche-Informationen
- risiken
title: Halluzinationen
type: risk
---

Halluzinationen sind ein Risiko, das beim Einsatz von KI auftreten kann, wenn KI-Systeme falsche oder ungenaue Informationen erzeugen.

### Beschreibung

Large Language Models (LLMs) neigen dazu, falsche Informationen zu produzieren, ein Phänomen, das als Halluzination bekannt ist. Während die KI Ergebnisse produzieren kann, die bemerkenswert aufschlussreich und nützlich erscheinen, **kann sie auch "Fakten" erfinden, die völlig plausibel erscheinen.** Diese Unwahrheiten sind in manchen Fällen nur schwer zu erkennen (1).


### Risikominimierung

Um Halluzinationen erkennen zu können, sollten die generierten Ergebnisse **überprüft** werden. Insbesondere bei Quellenangaben, Fakten oder Zitaten besteht eine hohe Wahrscheinlichkeit, dass diese falsch sind (1). 

Wenn die eigene Fachkenntnis nicht ausreicht, um den Wahrheitsgehalt zu beurteilen, müssen ggf. die **entsprechenden Quellen herangezogen** werden. 

Eine weitere Möglichkeit besteht darin, die KI zu bitten, ihre eigenen Antworten zu begründen (1). Durch **Nachfragen** können in manchen Fällen Unstimmigkeiten aufgedeckt werden.

Durch den gezielten Einsatz von **präzise formulierten Prompts** (Eingabeaufforderungen bei Chatbots) können ggf. Halluzinationen vermieden werden. Dies kann z.B. durch **klare Anweisungen** und die Bereitstellung ausreichender **Kontextinformationen** erreicht werden. Dadurch wird die Kommunikation mit dem Chatbot strukturierter und zielgerichteter.


---


## Quellen
1.	Mollick ER, Mollick L. Assigning AI: Seven Approaches for Students, with Prompts [Internet]. Rochester, NY; 2023 [zitiert 28. Juli 2023]. Verfügbar unter: https://papers.ssrn.com/abstract=4475995